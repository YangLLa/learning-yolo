[net]
# Testing
# batch=1
# subdivisions=1
# Training
batch=64         
subdivisions=16  
width=416
height=416
channels=3       
momentum=0.9     
decay=0.0005     
angle=0          
saturation = 1.5  
exposure = 1.5 
hue=.1    

learning_rate=0.001  
burn_in=1000   
max_batches = 500200 
policy=steps  
steps=400000,450000 
scales=.1,.1  


##stem block  stage 0
[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=16
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky



[route]
layers=-3

[maxpool]
stride=2
size=2
pad=2

[route]
layers=-1,-3


[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky

###stage1

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

###
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7



###

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7



[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[maxpool]
stride=2
size=2

##stage1end


###stage2
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

##
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

##
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[maxpool]
stride=2
size=2


### stage3

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7

##
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7


##
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7


##
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky



[route]
layers=-3

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1,-5,-7


[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# Downsample
[maxpool]
stride=2
size=2



############


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

### SPPF ###
[maxpool]
stride=1
size=5
pad=2

[maxpool]
stride=1
size=5
pad=2

[maxpool]
stride=1
size=5
pad=2

[route]
layers=-1,-2,-3,-4
### End SPPF ###


[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
# 在不改变网络特征图本身尺寸的情况下，利用1*1卷积通过filters数量来实现升维或降维
# 降维可以认为是通过减少冗余的特征图来降低模型中间层的权重的稀疏性，从而得到一个更加紧凑的网络结构


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

####

[route]
layers=64

#downsample  76*76 > 38*38
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky

[route]
layers=-1, 94


#num 100  38*38 + (76/2)*(76/2)


#downsaple  38*38  >  19*19
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky


[route]
layers = -1,104

####

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear

[yolo]
mask = 6,7,8
anchors = 104,20,  135,32,  95,161,  245,39,  334,43,  365,61,  339,81,  359,111,  346,171
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1


[route]
layers = -4

####

# 19*19 > 38*38

# 转置卷积替代临近上采样
#[deconvolutional]
#batch_normalize=1
#filters=512
#size=4
#stride=2
#pad = 1
#activation=leaky

[upsample]
stride=2

[route]
layers = -1,-9
####


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear

[yolo]
mask = 3,4,5
anchors = 104,20,  135,32,  95,161,  245,39,  334,43,  365,61,  339,81,  359,111,  346,171
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1


[route]
layers = -4


#### 38*38  >  76*76
#转置卷积替代临近上采样
#[deconvolutional]
#batch_normalize=1
#filters=256
#size=4
#stride=2
#pad=1
#activation=leaky

[upsample]
stride=2

[route]
layers = -1,64

####


[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear


[yolo]
mask = 0,1,2
anchors = 104,20,  135,32,  95,161,  245,39,  334,43,  365,61,  339,81,  359,111,  346,171
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
